{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax\n",
    "Below is the syntax for the gensim.utils.tokenize() function:\n",
    "\n",
    "```python\n",
    "gensim.utils.tokenize(text, lowercase=True, deacc=False, errors='strict', to_lower=False, lower=False)\n",
    "```\n",
    "\n",
    "text is the input text to be tokenized.\n",
    "\n",
    "lowercase is an optional parameter that specifies whether to convert the text to lowercase before tokenization. The default value is True.\n",
    "\n",
    "deacc is an optional parameter specifying whether to remove text accent marks. The default value is False.\n",
    "\n",
    "errors is an optional parameter that specifies how to handle decoding errors in the text. The default value is 'strict'.\n",
    "\n",
    "to_lower and lower are both optional parameters that are the same as lowercase and are used as a convenient alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner cases:\n",
    "joint → _joint \n",
    "jointed → _joint, ed \n",
    "disjointed → _di, s, jo, int, ed \n",
    "unisex → _un, ise, x \n",
    "true → _true \n",
    "untrue → _un, tr, ue \n",
    "estimate → _estimate \n",
    "overestimate → _over, est, imate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'educative', 'answers', 'joint', 'jointed', 'disjointed', 'unisex', 'true', 'untrue', 'estimate', 'overestimate']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "text = \"\"\" Welcome to Educative Answers.\n",
    "        joint\n",
    "        jointed \n",
    "        disjointed \n",
    "        unisex \n",
    "        true \n",
    "        untrue \n",
    "        estimate \n",
    "        overestimate \n",
    "        \"\"\"\n",
    "\n",
    "tokens = list(tokenize(text,lowercase=True))\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "en_stop = set(stopwords.words('english'))\n",
    "stopped_tokens = [word for word in tokens if word not in en_stop]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
