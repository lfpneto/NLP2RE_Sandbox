{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax\n",
    "Below is the syntax for the gensim.utils.tokenize() function:\n",
    "\n",
    "```python\n",
    "gensim.utils.tokenize(text, lowercase=True, deacc=False, errors='strict', to_lower=False, lower=False)\n",
    "```\n",
    "\n",
    "text is the input text to be tokenized.\n",
    "\n",
    "lowercase is an optional parameter that specifies whether to convert the text to lowercase before tokenization. The default value is True.\n",
    "\n",
    "deacc is an optional parameter specifying whether to remove text accent marks. The default value is False.\n",
    "\n",
    "errors is an optional parameter that specifies how to handle decoding errors in the text. The default value is 'strict'.\n",
    "\n",
    "to_lower and lower are both optional parameters that are the same as lowercase and are used as a convenient alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner cases:\n",
    "joint → _joint \n",
    "jointed → _joint, ed \n",
    "disjointed → _di, s, jo, int, ed \n",
    "unisex → _un, ise, x \n",
    "true → _true \n",
    "untrue → _un, tr, ue \n",
    "estimate → _estimate \n",
    "overestimate → _over, est, imate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'educative', 'answers', 'joint', 'jointed', 'disjointed', 'unisex', 'true', 'untrue', 'estimate', 'overestimate']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "text = \"\"\" Welcome to Educative Answers.\n",
    "        joint\n",
    "        jointed \n",
    "        disjointed \n",
    "        unisex \n",
    "        true \n",
    "        untrue \n",
    "        estimate \n",
    "        overestimate \n",
    "        \"\"\"\n",
    "\n",
    "tokens = list(tokenize(text,lowercase=True))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### dif to NLTK\n",
    " - NLTK does not remove dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['welcome', 'to', 'educative', 'answers', '.', 'joint', 'jointed', 'disjointed', 'unisex', 'true', 'untrue', 'estimate', 'overestimate']]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize documents\n",
    "text_list = []\n",
    "text_list.append(text)\n",
    "tokens_nltk = [word_tokenize(doc.lower()) for doc in text_list]\n",
    "\n",
    "print(tokens_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'educative', 'answers', 'joint', 'jointed', 'disjointed', 'unisex', 'true', 'untrue', 'estimate', 'overestimate']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_costum_fn = tokenizer.tokenize(text.lower())\n",
    "\n",
    "print(tokens_costum_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
