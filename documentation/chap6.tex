\chapter{Evaluation} \label{chap:chap6}

To evaluate the result, we will evaluate the framework capacity to identify requierments subjected to change by the introduction of a new requirement.

To evaluate the perfomrance we will refer to the document Functional Requirement Specifications for the ERTMS/ETCS System in Finland (ETCS FI-FRS), 2018-12-14</issue_date>, as it is publicly acssessible. 


The (ETCS FI-FRS) has been ordered by Finnish Transport Agency and written by railway experts from Proxion Plan Oy in close cooperation with the Finnish
Transport Safety Agency Trafi, VR-Group Ltd. and rolling stock and equipment suppliers (Siemens, Ansaldo STS).

The 2007 ERTMS XML document and the 2018 FRS XML document both focus on European Train Control System (ETCS) specifications, but they target different aspects and contexts. 
Specific functional requirements for ETCS implementation in Finland.
Introduction tailored to Finnish ETCS requirements.
Focuses on compliance with Finnish national regulations.
Provides a brief summary of ETCS Level 2 in Finland.


Using the (ETCS FI-FRS), a new requirement has to created:

OPTION 1: The chosen baseline for the ETCS implementation in Finland is Baseline 3, with Level 2 functionality. This choice ensures compatibility with other European implementations and supports advanced functionalities such as continuous communication and full supervision.

OPTION 2: National ETCS Parameters. The following national ETCS parameters are defined for Finland, Q_NVDRIVER_ADHES, 1, Driver adhesion factor. Q_NVGUARDBRAKE, 0, Guard brake enabled. V_NVSHUNT, 30, Maximum shunting speed in km/h


Having identified the proceudure, lets run the framework and evaluate the results:

1.
start by creating a model using 3 data sources :
- 2006-eirene_sys_15.xml 
- 2007-eirene_fun_7-2.xml 
- 2007-ertms.xml


Now lets see the data cleaning strategy, figure 1 
2. Run the model creation with the following parameters: 
"n_gram_min": 1,
"n_gram_max": 1,
no removal of stopwords 
no stemming or lemming
using LDA with hard clustering (only one topic per req)

lets analise the results
figure 2 shows the results, the requirement, the cleaned up text and the topic identified to the requierment.
taking the example of req_id : "1.4.1.4", "10.2.1", "10.3.2"
"text_clean" shows us the result of the data cleaning process. This is the tokenized data used to create the model. 
As we can see, by not removing stopwords in the cleaned data, the topics identified contain words such as "it" that do not had value to the topic, as well as isolated numbers.
Human Evaluation: Directly assesses the comprehension and usefulness of the topics through human ratings.
The results that we read in the topic have nothing to do with what is in the requierments, meaning that topic creation has not been sucessful.


the distance between topics is calculated by a cosine similarity matrix. topics with high proximity mean that they are close in meaning or understanding and should be evaluated. As an example, in figure 3, with a relatively low threshold, all topics with a inverse distance (closeness) higher than 30\% where registered. In the example of pai 1, we can see that the tokens "emergency" "shunting" and "railway" are repeated.
Having unique topics is a good metric of a model.


one of the option that could be used to clean the topic from not meaningfull words would be to set a parameter in lda to discard from a topic words that have a a probability lower than a certain value, but has it can be seen in figure 2, there is not a wde enough distribuiton of values so as to define an accurate threshold to discard some words, e.g. "it" in topic_id 44 has a weight of 0.027, by discarding words with aweight equal or lower to 0.27 would discard "transmitted" "order" and "possible" from topic_id 26, words that can add meaning to the topic. 

lets do another run by removing stopwords.
after removing the stopword and looking at the topics we can see that the word shall is present in almost all topics (figure 4). this is a context specific work, as it is comonly used in requierments.
Such word can be removed, as they have a high weighting in the topic but do not add anything to to topic syntax comprehensionAdition workds have also been considered to be removed (figure 5). The selection was done by retrieving from the creaated dictionary the most frequent tokens and seeing wich ones did not add meaning to a topic, by theyr ambiguity. 


call and calls






Examples of external metrics include:
• Human Evaluation: Directly assesses the comprehension and usefulness of the topics through human ratings.
• Semantic Coherence: Evaluates how well the topics match human judgments of semantic relatedness.
• Held-out Likelihood: Measures the likelihood of held-out documents, with higher likelihood indicating better generalization.
• Downstream Task Performance: Evaluates how well the topic model performs on extrinsic tasks like text classification or retrieval.



\section{Model Creation}
\section{Using the models}
Perplexity: Measures how well a probabilistic model predicts a new sample of data, with
lower perplexity indicating better model fit.


% #NOTE: - The importance of evaluating the performance of a topic modeling algorithm 
Metrics are indispensable for evaluating the results of machine learning techniques because they provide a quantitative basis for assessing the quality, validity, and efficiency of the models. They facilitate comparison, guide model selection and tuning, ensure interpretability, and help build trust in the models. 

%To validate the effectiveness and applicability of the developed model
This chapter provides a broad overview of metrics for evaluation of unsupervised models. The primary goal of the evaluation is to asses the performance of the proposed solution, as the evaluation is registered for different parameters as the model is further scaled, to measure the quality of the topics generated, ensure the model's robustness, and explore the interpretability and distinctiveness of the topics. 

% Internal metrics, such as perplexity and coherence scores, provide quantitative measures of the model's performance using data intrinsic to the model. External metrics, including human judgment and comparisons to known labels, offer qualitative insights into the model's interpretability and practical relevance.

In addition to the commonly used metrics, this chapter also discusses potential evaluation methods to be explored for improvement.

% The structure of this chapter is as follows: Section 2 details the internal evaluation metrics used, including perplexity and coherence scores. Section 3 presents the results of the external evaluation, highlighting the findings from human judgment and ground truth comparisons. Section 4 explores additional methods that were considered for future exploration, providing a foundation for continued research and improvement. Finally, Section 5 summarizes the evaluation findings and discusses their implications for the overall performance of the topic modeling algorithm.

% Hyperparameter Tuning
%Metrics are essential for tuning hyperparameters of unsupervised learning algorithms. For instance, the number of clusters in k-means can be chosen based on metrics like the Elbow Method or Silhouette Score.

\section{Registering results}
% How we are registering and gathering metrics



A record of parameters, configurations and obtained metrics is needed in each run of model creation, for evaluating the quality of the models throughout it's evolution. When a  model is established, the developed tool outputs a JSON file containing parameters used in model creation and results from artifacts analysis.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/EvaluationResults_1png.png}
    \caption{Evaluation results: Summary of topics}
    \label{fig:EvaluationResults_1png}
\end{figure}

Topic are organized by ID, containing their corresponding keywords with respective weights, IDs of requirements linked to each topic (grouped by artifact) and the records for total number of requirements per topic and artifact. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/EvaluationResults_2png.png}
    \caption{Evaluation results: Requirement analysis}
    \label{fig:EvaluationResults_2png}
\end{figure}

For each requirement processed, the tool registers ID, text, tokenized text, associated topic ID, and topic keywords, subsequently compiling this structured information into the JSON format. 

This approach ensures that data is neatly organized and accessible. Saving these metrics in JSON format facilitates easy sharing and further exploration of model outcomes, enabling comprehensive analysis and interpretation of results.


